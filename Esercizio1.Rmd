---
title: "Exercise1"
author: "Michele Luca Puzzo, Marco Muscas, Shokoufeh Mansourihafshejani"
date: "11/22/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

## 1.1
Firstly we want to show how the algorithm computes the vector **y** at the first step. It is initialized as vector of zeros, it is p-dimensional: 

$\textbf{y}= (y_1, ..., y_i, ..., y_p) \hspace{0.2cm}$

We are computing the i-th element of the vector **y** at first step. We observe that the frequency vector in the first step is made by all zeros except that for the j-th element that is one.
So it comes up, multiplying the i-th row of L with x, that i-th element of y is equal to:
$$ y^{(1)}_i= \sum_{r=1}^d L_{ir} \cdot x^{(1)}_r = L_{ij} \cdot x^{(1)}_j = L_{ij} \cdot 1 = L_{ij}, \hspace{0.2cm} i\in \{1,...p\} $$
So doing this computation for all the p rows of L, at the end of the first step **y** will be equal to the column j-th of the random matrix: 

$\mathbf{y^{(1)}}= \mathbf{L_{:j}}$

At time step k-1, j-th coordinate of **x^(k−2)^** increases by 1 to get **x^(k-1)^**. We call **f** the update of the frequency vector, made by all zeros except for the position j-th. We can break down the frequency vector at the step k-1 as sum of the frequency vector at step k-2 plus the update **f**
We know that the matrix multiplication has the distributive property respect to the sum so we can write:
$$ \mathbf{y}^{(k-1)} = L \cdot \mathbf{x}^{(k-1)} = L \cdot (\mathbf{x}^{(k-2)} + \mathbf{f}) = L \cdot \mathbf{x}^{(k-2)} + L \cdot  \mathbf{f} =  \mathbf{y}^{(k-2)} + \mathbf{L_{:j}}$$
The first addend was for the definition the vector **y** at the step k-1 while the second addend is how the column j-th of the matrix L: we have shown this computing the first step. 

So we have shown that increases implicitly the j-th coordinate of $\mathbf{x}^{(k-2)}$ by 1 to get $\mathbf{x}^{(k-1)}$ corresponds to an explicit change to obtain $\mathbf{y}^{(k-1)}$ is to add the j-th column of L to $\mathbf{y}^{(k-2)}$.

## 1.2

We have created a function called **simulation** in which we have performed the randomized algorithm:

we have to compute the frequency vector to compute its norm to check the *Johnson-Lindenstrauss lemma* but in the randomized algorithm we will not use it. We have decided to change the raw stream between the different simulations but it makes no difference because the probability in *JL lemma* simply accounts for the uncertainty implied by the randomness of L. The raw stream has length n, but its indexes can vary between 1 and d and they are integer values. 

In the algorithm **y** will not see the entire raw stream but at each step it will receive just one index to respect the very idea of a streaming algorithm. 

We initialize **y** as a p-dimensional vector of zeros; 
at the beginning of each simulation run we compute the random projection matrix (p x d) that is whose entries are drawn independently as N~1~(0, 1/p) so we use the function *rnorm* to generate its p$\cdot$d values. 

At each step k we pick the k-th element of the raw stream that is a number j varying between 1 and d and we update the vector **y** adding to it the j-th of L. So in each step we do not see the entire raw stream but just one value! In each step we update just the vector **y**

At the end of each simulation run we compute the norm of **y** and if its value is between
$$ (1- \epsilon) \cdot \|\textbf{x}\| \le \|\textbf{y}\| \le (1 + \epsilon) \cdot \|\textbf{x}\|$$ 
then we increase by one the integer *counts* that counts in how many simulation run the event of JL lemma is respected, so it counts how many times this event occurs in M simulation runs.  
The output of *simulation* is the ratio between *counts* over the number of runs *M*.

```{r}
M <- 1000
simulation <- function(p, d, n, eps){
  #initialization of x: it has length d.  
  x <- vector(mode = "integer", length = d)
  #raw stream, sampling uniformly from 1 to d 
  D_n <- sample(1:d, n, replace = T)
  for (i in D_n){
    #update sequentially of the frequency vector 
    x[i] <- x[i] + 1 
  }
#norm 2 of the frequency vector
  norm_x <- norm(x, type ="2")
  

  counts <- 0
  for (r in 1:M){
    #compute matrix L, one for each simulation 
    L <- matrix(rnorm(p*d, 0, sd = sqrt(1/p)), nrow = p, ncol = d)
    y <- vector(mode = "double", length = p)
    for (k in 1:n){
      #update the vector y
      y <- y + L[,D_n[k]]
    }
    #norm 2 of y
    norm_y <- norm(y, type = "2")
    #check the value of the norm of y
    if ((norm_y> (1-eps) * norm_x) & (norm_y < (1+eps) * norm_x)){
    counts <- counts + 1 
    }
  }
  #how many times in M simulation norm of y respect the condition
  return (counts)
}
```

We have decided to run 7 different simulation in which we change a bit the values of *d*, the size of the “data-alphabet”, *n*, size of the raw stream, the tolerance *epsilon* and *p*, the size of the projection.
We have always chosen d larger than n of one order of magnitude, 10 times larger. Instead p ignores completely
the alphabet size but it has to be much smaller than n: $d\gg n \gg p$

For the sixth value of p we have taken log(n)/($\epsilon^2$) so $\mathbf{y}^{(k)}$ gives an accurate estimate at each time steps, while for the others values we have taken p of the order of 1/$\epsilon^2$. In the big O notation it means that p is bounded above by this values (up to constant factor that we have chosen equals to log(t) with t $\in$ {e,54,4,10,20}) asymptotically. We have chosen this values for the constant factor so to have different value of p to compare among them keeping constant the tolerance or the JL probability threshold.

At the end of the stream,$\|\textbf{y}\|$  gives a (1 + $\epsilon$) approximation of $\|\textbf{x}\|$ with constant probability.

So to summarize we have picked seven different values of p, but just two different values of d, and n. We put our values in a data.frame to a better visualization. 

```{r}
#Decision of the parameters.  
d <- c(10^4, 10^4, 10^5, 10^5, 10^5, 10^5, 10^5)
n <- c(10^3, 10^3, 10^4, 10^4, 10^4, 10^4, 10^4)
eps <- c(0.1, 0.2, 0.1, 0.1, 0.1, 0.1, 0.06)
p <- c(round(1/eps[1]^2), round(log(54)/eps[2]^2), round(log(4)/eps[3]^2),round(log(10)/eps[4]^2), round(log(20)/eps[5]^2),round(log(n[6])/eps[6]^2), round(log(20)/eps[7]^2))

#put them in a data.frame
parameters <- data.frame("d"= d,"n" = n, "p" = p, "Tolerance" = eps) 
parameters
```

Now we run our simulations and we also compute for the different values of tolerance and p the values whose JL probability has to be greater: $(1- e^{-\epsilon^2 \cdot p})$

```{r}
sim <- c(length = 7)
JL <- c(length = 7)

for (i in 1:7){
  #run the simulation for each of the value of p,d,n and eps
  sim[i] <- simulation(p[i], d[i], n[i], eps[i])
  #threshold of JL lemma
  JL[i] <- 1 - exp(-p[i]*eps[i]^2)

}
#put the results of simulation in the data.frame
sim_1 <- factor(sim/M)
JL_1 <- factor(JL)
parameters$Simulation <- sim_1
parameters$JL_Threshold <- JL_1
```

We want to visualize for each simulation how many runs confirm the JL lemma. 

```{r}
x <- seq(7) # label of the bars
barplot(sim, col ="purple", xlab="N. of simulation", ylab="N. of runs in which JL lemma is confirmed", main = "Good runs", names.arg = x)
```

We want to visualize how the JL probability lower bound varies as function of the size of the projection keeping constant the tolerance, n and d. To do this we have run four simulation (from the third to the sixth). We see that as expected increasing the number of p, the lower bound of probability that $(1- \epsilon) \cdot \|\textbf{x}\| \le \|\textbf{y}\| \le (1 + \epsilon) \cdot \|\textbf{x}\|$ increases, not in a linear way since its expression, given our choice of p (we simplify $\epsilon^2$), is $1-e^{-log(t)}=1-1/t$.  

```{r}
plot(p[3:6], JL[3:6], main="Trend of JL probability as s function of p", xlab= "p:size of the projection", ylab="JL probability", col="salmon", pch = 19)
xtick<-seq(100, 1000, by=50)
axis(side=1, at=xtick, labels = T)
y <- function(x) 1-1/x #due to the choice of p of the order of 1/eps^2
x<-seq(4,54,by=1) #choice of the constant the we have assigned to p 
par(new=TRUE) #admit two plot together
plot(x,y(x), type = "l", col="orchid", lwd = 2, axes = FALSE, xlab="", ylab="")
```

Finally we want to see if JL lemma is checked by our simulations:

```{r}
check <- vector(mode = "character", length = 7)
#check our simulation 
for(i in 1:7){
  if (sim[i]/M >= JL[i]){
    check[i] <- "TRUE"
  }else{
    check[i] <- "FALSE"
  }
}
#put the response in data.frame
check <- factor(check)
parameters$Check <- check
parameters
```

From our first two simulations we can see that keeping constant the number of p, d, n and but increasing tolerance from 0.1 to 0.2 allow to JL probability lower bound to increase (this confirm the natural intuition: if the norm of **y** can move away much from norm of **x** the probability of the event improves. It's a sort of sanity check). 

In the last simulation we have picked the same JL probability lower bound of the fifth simulation to see how much p has to increase to decrease tolerance of 0.04.
Other simulation was useful to see the trend of JL probability as function of p. 

## 1.3

We have achieved our goal because we have implemented the algorithm updating only the vector **y**. Moreover we have set p to a value of the order 1/$\epsilon$^2, in one case also put it equals to log(n)/$\epsilon$^2. We have picked epsilon such that p is always smaller than n of at least one order of magnitude, but epsilon is small enough to guarantee an accurate approximation of $\|\textbf{x}\|$ , like (1 $\pm$ 0.1) $\cdot$ $\|\textbf{x}\|$. Furthermore **y** differently of the frequency vector has in this way a reduced dimension. 

Setting in this way p *JL lemma* guarantees for its event a probability circa equals to one, like 0.99, but in spite of that our simulations confirm the lemma because they always give back a probability greater than this value. 


