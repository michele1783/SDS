---
title: "Exercise1"
author: "Michele Luca Puzzo, Marco Muscas, Shokoufeh Mansourihafshejani"
date: "11/22/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1.1
Firstly we want to show how the algorithm computes the vector **y** at the first step. It is initialized as vector of zeros, it is p-dimensional. 
$$ \textbf{y}= (y_1, ..., y_i, ..., y_p) \hspace{0.2cm}$$
We are computing the i-th element of the vector **y** at first step. We observe that the frequency vector in the first step is made by all zeros except that for the j-th element that is one.
So it comes up that i-th element of y is equal to:
$$ y^{(1)}_i= \sum_{r=1}^p L_{ir} \cdot x^{(1)}_r = L_{ij} \cdot x^{(1)}_j = L_{ij} \cdot 1 = L_{ij}, \hspace{0.2cm} i\in \{1,...p\} $$
So at the end of the first step **y** will be equal to the column j-th of the random matrix: 
$$ \mathbf{y^{(1)}}= \mathbf{L_{:j}} $$

At time step k-1, j-th coordinate of **x^(k−2)^** increases by 1 to get **x^(k-1)^**. We call **f** the update of the frequency vector, made by all zeros except for the position j-th. We can break down the frequency vector at the step k-1 as sum of the frequency vector at step k-2 plus the update **f**
We know that the matrix multiplication has the distributive function respect to the sum so we can write:
$$ \mathbf{y^{k-1}} = L \cdot \mathbf{x^{k-1}} = L \cdot (\mathbf{x^{k-2}} + \mathbf{f}) = L \cdot \mathbf{x^{k-2}} + L \cdot  \mathbf{f} =  \mathbf{y^{k-2}} + \mathbf{L_{:j}}$$
The first addend was for the definition the vector **y** at the step k-1 while the second addend is how the column j-th of the matrix L: we have shown this computing the first step. 

## 1.2

I have to compute the frequency vector to compute its norm to check the *Johnson-Lindenstrauss lemma* but in the randomized algorithm we will not use it. We have decided to change the raw stream between the different simulations but it makes no difference because the probability in *JL lemma* simply accounts for the uncertainty implied by the randomness of L. The raw stream has length n, but its indexes can vary between 1 and d and they are integer values. 

In the algorithm **y** will not see the entire raw stream but at each step it will receive just one index to respect the very idea of a streaming algorithm. 


We have created a function called **simulation** in which we have performed the randomized algorithm:

we initialize **y** as a p-dimensional vector of zeros; 
at the beginning of each simulation run we compute the random projection matrix (p x d) that is whose entries are drawn independently as N~1~(0, 1/p) so we use the function *rnorm* to generate its p*d values. 

At each step k we pick the k-th element of the raw stream that is a number j varying between 1 and d and we update the vector **y** adding to it the j-th of L. So in each step we do not see the entire raw stream but just one value! In each step we update just the vector **y**

At the end of each simulation run we compute the norm of **y** and if its value is between
$$ (1- \epsilon) \cdot \|{x} \| \le \|{y} \| \le (1 + \epsilon) \cdot \|{x} \|)$$ 
then we increase by one the integer *counts* that counts in how many simulation run the JL lemma is respected. 

The output of simulation is the ratio between *counts* over the number of runs *M*.

```{r}
M <- 1000
simulation <- function(p, d, n, eps){
  #initialization of x: it has length d.  
  x <- vector(mode = "integer", length = d)
  #raw stream 
  D_n <- sample(1:d, n, replace = T)
  for (i in D_n){
    #update sequentially of the frequency vector 
    x[i] <- x[i] + 1 
  }ì
#norm 2 of the frequency vector
  norm_x <- norm(x, type ="2")
  

  counts <- 0
  for (r in 1:M){
    #compute matrix L, one for each simulation 
    L <- matrix(rnorm(p*d, 0, sd = sqrt(1/p)), nrow = p, ncol = d)
    y <- vector(mode = "double", length = p)
    for (k in 1:n){
      #update the vector y
      y <- y + L[,D_n[k]]
    }
    norm_y <- norm(y, type = "2")
    if ((norm_y> (1-eps) * norm_x) & (norm_y < (1+eps) * norm_x)){
    counts <- counts + 1 
    }
  }

  return (counts/M)
}
```

We have decided to run 3 different simulation in which we change a bit the values of *d*, the size of the “data-alphabet”, *n*, size of the raw stream, the tolerance *epsilon* and *p*, the size of the projection.
Since we have always chosen d larger than n of one order of magnitude. Instead p ignores completely
the alphabet size  but it has to be much smaller than n. 
$$d\gg n \gg p $$
For the first value of p we have taken log(n)/\(\epsilon\)^2^ so to have y^(k)^ that gives an accurate estimate at each time steps, while for the second and the third values we have taken p of the order of 1/\(\epsilon\)^2^ so at the end of the stream, gives a (1 + \(\epsilon\)) approximation of $|x|$ with constant probability.

So to summarize we have picked four different values of p, but just two different values of p, n and epsilon. We put our values in a data.frame to a better visualization. 

```{r}
#Decision of the parameters.  
d <- c(10^5, 10^5, 10^6)
n <- c(10^4, 10^4, 10^5)
eps <- c(0.1, 0.075, 0.05)
p <- round(c(round(log(n[1])/eps[1]^2), round(log(100)/eps[2]^2), round(log(1000)/eps[3]^2)))
#put them in a data.frame
parameters <- data.frame("d"=d,"n" = n, "p"= p, "Tolerance"= eps) 
parameters
```

Now we run our simulations and we also compute for the different values of tolerance and p the values whose JL probability has to be greater. 

$$ (1- e^{-\epsilon^2 \cdot p}) $$
```{r}
sim <- c(length = 3)
JL <- c(length = 3)

for (i in 1:3){
  sim[i] <- simulation(p[i], d[i], n[i], eps[i])
  JL[i] <- 1 - exp(-p[i]*eps[i]^2)

}
sim_1 <- factor(sim)
JL_1 <- factor(JL)
parameters$Simulation <- sim_1
parameters$JL <- JL_1
parameters

```

Finally we want to see if JL lemma is checked by our simulation. We cannot use ">" to compare the value obtained by the simulation and the theoretical value because the last one is really near to one, so we take the max and then with function *identical* we compare them. We put all values in data.frame for have a complete vision of our results. 

```{r}
check <- vector(mode = "character", length = 3)
for(i in 1:3){
  mas <- max(sim[i], JL[i])
  if (identical(sim[i], mas)){
    check[i] <- "TRUE"
  }else{
    check <- "FALSE"
  }
}
check <- factor(check)
parameters$Check <- check
parameters
```

## 1.3

We have achieved our goal because we have implemented the algorithm updating only the vector **y**. Moreover we have set p to a value of the order 1/\epsilon^2, in one case also put it equals to log(n)/\epsilon^2. Setting in this way p *JL lemma* guarantees a probability circa equals to one, like 0.999. Our simulations confirm the lemma! 


